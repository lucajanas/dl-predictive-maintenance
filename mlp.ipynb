{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Predictive Maintenance\n",
    "Data set: https://archive.ics.uci.edu/dataset/316/condition+based+maintenance+of+naval+propulsion+plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lp</th>\n",
       "      <th>v</th>\n",
       "      <th>GTT</th>\n",
       "      <th>GTn</th>\n",
       "      <th>GGn</th>\n",
       "      <th>Ts</th>\n",
       "      <th>Tp</th>\n",
       "      <th>T48</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>P48</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>Pexh</th>\n",
       "      <th>TIC</th>\n",
       "      <th>mf</th>\n",
       "      <th>kMc</th>\n",
       "      <th>kMt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.138</td>\n",
       "      <td>3</td>\n",
       "      <td>289.964</td>\n",
       "      <td>1349.489</td>\n",
       "      <td>6677.380</td>\n",
       "      <td>7.584</td>\n",
       "      <td>7.584</td>\n",
       "      <td>464.006</td>\n",
       "      <td>288</td>\n",
       "      <td>550.563</td>\n",
       "      <td>1.096</td>\n",
       "      <td>0.998</td>\n",
       "      <td>5.947</td>\n",
       "      <td>1.019</td>\n",
       "      <td>7.137</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.088</td>\n",
       "      <td>6</td>\n",
       "      <td>6960.180</td>\n",
       "      <td>1376.166</td>\n",
       "      <td>6828.469</td>\n",
       "      <td>28.204</td>\n",
       "      <td>28.204</td>\n",
       "      <td>635.401</td>\n",
       "      <td>288</td>\n",
       "      <td>581.658</td>\n",
       "      <td>1.331</td>\n",
       "      <td>0.998</td>\n",
       "      <td>7.282</td>\n",
       "      <td>1.019</td>\n",
       "      <td>10.655</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.144</td>\n",
       "      <td>9</td>\n",
       "      <td>8379.229</td>\n",
       "      <td>1386.757</td>\n",
       "      <td>7111.811</td>\n",
       "      <td>60.358</td>\n",
       "      <td>60.358</td>\n",
       "      <td>606.002</td>\n",
       "      <td>288</td>\n",
       "      <td>587.587</td>\n",
       "      <td>1.389</td>\n",
       "      <td>0.998</td>\n",
       "      <td>7.574</td>\n",
       "      <td>1.020</td>\n",
       "      <td>13.086</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.161</td>\n",
       "      <td>12</td>\n",
       "      <td>14724.395</td>\n",
       "      <td>1547.465</td>\n",
       "      <td>7792.630</td>\n",
       "      <td>113.774</td>\n",
       "      <td>113.774</td>\n",
       "      <td>661.471</td>\n",
       "      <td>288</td>\n",
       "      <td>613.851</td>\n",
       "      <td>1.658</td>\n",
       "      <td>0.998</td>\n",
       "      <td>9.007</td>\n",
       "      <td>1.022</td>\n",
       "      <td>18.109</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.140</td>\n",
       "      <td>15</td>\n",
       "      <td>21636.432</td>\n",
       "      <td>1924.313</td>\n",
       "      <td>8494.777</td>\n",
       "      <td>175.306</td>\n",
       "      <td>175.306</td>\n",
       "      <td>731.494</td>\n",
       "      <td>288</td>\n",
       "      <td>645.642</td>\n",
       "      <td>2.078</td>\n",
       "      <td>0.998</td>\n",
       "      <td>11.197</td>\n",
       "      <td>1.026</td>\n",
       "      <td>26.373</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lp   v        GTT       GTn       GGn       Ts       Tp      T48   T1  \\\n",
       "0  1.138   3    289.964  1349.489  6677.380    7.584    7.584  464.006  288   \n",
       "1  2.088   6   6960.180  1376.166  6828.469   28.204   28.204  635.401  288   \n",
       "2  3.144   9   8379.229  1386.757  7111.811   60.358   60.358  606.002  288   \n",
       "3  4.161  12  14724.395  1547.465  7792.630  113.774  113.774  661.471  288   \n",
       "4  5.140  15  21636.432  1924.313  8494.777  175.306  175.306  731.494  288   \n",
       "\n",
       "        T2    P48     P1      P2   Pexh     TIC     mf   kMc    kMt  \n",
       "0  550.563  1.096  0.998   5.947  1.019   7.137  0.082  0.95  0.975  \n",
       "1  581.658  1.331  0.998   7.282  1.019  10.655  0.287  0.95  0.975  \n",
       "2  587.587  1.389  0.998   7.574  1.020  13.086  0.259  0.95  0.975  \n",
       "3  613.851  1.658  0.998   9.007  1.022  18.109  0.358  0.95  0.975  \n",
       "4  645.642  2.078  0.998  11.197  1.026  26.373  0.522  0.95  0.975  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['lp','v','GTT','GTn','GGn','Ts','Tp','T48','T1','T2','P48','P1','P2','Pexh','TIC','mf','kMc','kMt']\n",
    "data = pd.read_csv('data/data.csv', names=columns, skiprows=1)\n",
    "\n",
    "# kMc = GT Compressor decay state coefficient, kMt = GT Turbine decay state coefficient\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11934, 18)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lp</th>\n",
       "      <th>v</th>\n",
       "      <th>GTT</th>\n",
       "      <th>GTn</th>\n",
       "      <th>GGn</th>\n",
       "      <th>Ts</th>\n",
       "      <th>Tp</th>\n",
       "      <th>T48</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>P48</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>Pexh</th>\n",
       "      <th>TIC</th>\n",
       "      <th>mf</th>\n",
       "      <th>kMc</th>\n",
       "      <th>kMt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.0</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.167</td>\n",
       "      <td>15.000</td>\n",
       "      <td>27247.499</td>\n",
       "      <td>2136.289</td>\n",
       "      <td>8200.947</td>\n",
       "      <td>227.336</td>\n",
       "      <td>227.336</td>\n",
       "      <td>735.495</td>\n",
       "      <td>288.0</td>\n",
       "      <td>646.215</td>\n",
       "      <td>2.353</td>\n",
       "      <td>0.998</td>\n",
       "      <td>12.297</td>\n",
       "      <td>1.029</td>\n",
       "      <td>33.641</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.626</td>\n",
       "      <td>7.746</td>\n",
       "      <td>22148.613</td>\n",
       "      <td>774.084</td>\n",
       "      <td>1091.316</td>\n",
       "      <td>200.496</td>\n",
       "      <td>200.496</td>\n",
       "      <td>173.681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.676</td>\n",
       "      <td>1.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.337</td>\n",
       "      <td>0.010</td>\n",
       "      <td>25.841</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.138</td>\n",
       "      <td>3.000</td>\n",
       "      <td>253.547</td>\n",
       "      <td>1307.675</td>\n",
       "      <td>6589.002</td>\n",
       "      <td>5.304</td>\n",
       "      <td>5.304</td>\n",
       "      <td>442.364</td>\n",
       "      <td>288.0</td>\n",
       "      <td>540.442</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.998</td>\n",
       "      <td>5.828</td>\n",
       "      <td>1.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.144</td>\n",
       "      <td>9.000</td>\n",
       "      <td>8375.884</td>\n",
       "      <td>1386.758</td>\n",
       "      <td>7058.324</td>\n",
       "      <td>60.317</td>\n",
       "      <td>60.317</td>\n",
       "      <td>589.873</td>\n",
       "      <td>288.0</td>\n",
       "      <td>578.092</td>\n",
       "      <td>1.389</td>\n",
       "      <td>0.998</td>\n",
       "      <td>7.447</td>\n",
       "      <td>1.020</td>\n",
       "      <td>13.678</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.140</td>\n",
       "      <td>15.000</td>\n",
       "      <td>21630.659</td>\n",
       "      <td>1924.326</td>\n",
       "      <td>8482.082</td>\n",
       "      <td>175.268</td>\n",
       "      <td>175.268</td>\n",
       "      <td>706.038</td>\n",
       "      <td>288.0</td>\n",
       "      <td>637.142</td>\n",
       "      <td>2.083</td>\n",
       "      <td>0.998</td>\n",
       "      <td>11.092</td>\n",
       "      <td>1.026</td>\n",
       "      <td>25.276</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.148</td>\n",
       "      <td>21.000</td>\n",
       "      <td>39001.427</td>\n",
       "      <td>2678.079</td>\n",
       "      <td>9132.606</td>\n",
       "      <td>332.365</td>\n",
       "      <td>332.365</td>\n",
       "      <td>834.066</td>\n",
       "      <td>288.0</td>\n",
       "      <td>693.924</td>\n",
       "      <td>2.981</td>\n",
       "      <td>0.998</td>\n",
       "      <td>15.658</td>\n",
       "      <td>1.036</td>\n",
       "      <td>44.552</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.300</td>\n",
       "      <td>27.000</td>\n",
       "      <td>72784.872</td>\n",
       "      <td>3560.741</td>\n",
       "      <td>9797.103</td>\n",
       "      <td>645.249</td>\n",
       "      <td>645.249</td>\n",
       "      <td>1115.797</td>\n",
       "      <td>288.0</td>\n",
       "      <td>789.094</td>\n",
       "      <td>4.560</td>\n",
       "      <td>0.998</td>\n",
       "      <td>23.140</td>\n",
       "      <td>1.052</td>\n",
       "      <td>92.556</td>\n",
       "      <td>1.832</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              lp          v        GTT        GTn        GGn         Ts  \\\n",
       "count  11934.000  11934.000  11934.000  11934.000  11934.000  11934.000   \n",
       "mean       5.167     15.000  27247.499   2136.289   8200.947    227.336   \n",
       "std        2.626      7.746  22148.613    774.084   1091.316    200.496   \n",
       "min        1.138      3.000    253.547   1307.675   6589.002      5.304   \n",
       "25%        3.144      9.000   8375.884   1386.758   7058.324     60.317   \n",
       "50%        5.140     15.000  21630.659   1924.326   8482.082    175.268   \n",
       "75%        7.148     21.000  39001.427   2678.079   9132.606    332.365   \n",
       "max        9.300     27.000  72784.872   3560.741   9797.103    645.249   \n",
       "\n",
       "              Tp        T48       T1         T2        P48         P1  \\\n",
       "count  11934.000  11934.000  11934.0  11934.000  11934.000  11934.000   \n",
       "mean     227.336    735.495    288.0    646.215      2.353      0.998   \n",
       "std      200.496    173.681      0.0     72.676      1.085      0.000   \n",
       "min        5.304    442.364    288.0    540.442      1.093      0.998   \n",
       "25%       60.317    589.873    288.0    578.092      1.389      0.998   \n",
       "50%      175.268    706.038    288.0    637.142      2.083      0.998   \n",
       "75%      332.365    834.066    288.0    693.924      2.981      0.998   \n",
       "max      645.249   1115.797    288.0    789.094      4.560      0.998   \n",
       "\n",
       "              P2       Pexh        TIC         mf        kMc        kMt  \n",
       "count  11934.000  11934.000  11934.000  11934.000  11934.000  11934.000  \n",
       "mean      12.297      1.029     33.641      0.662      0.975      0.988  \n",
       "std        5.337      0.010     25.841      0.507      0.015      0.008  \n",
       "min        5.828      1.019      0.000      0.068      0.950      0.975  \n",
       "25%        7.447      1.020     13.678      0.246      0.962      0.981  \n",
       "50%       11.092      1.026     25.276      0.496      0.975      0.988  \n",
       "75%       15.658      1.036     44.552      0.882      0.988      0.994  \n",
       "max       23.140      1.052     92.556      1.832      1.000      1.000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.95\n",
       "1        0.95\n",
       "2        0.95\n",
       "3        0.95\n",
       "4        0.95\n",
       "         ... \n",
       "11929    1.00\n",
       "11930    1.00\n",
       "11931    1.00\n",
       "11932    1.00\n",
       "11933    1.00\n",
       "Name: kMc, Length: 11934, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.kMc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decay_status_kMc\n",
       "done            5850\n",
       "decaying        3510\n",
       "not decaying    2574\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a custom function to determine decay status\n",
    "def get_decay_status(coeff):\n",
    "    if coeff < 0.975:\n",
    "        return 'done'\n",
    "    elif 0.975 <= coeff < 0.99:\n",
    "        return 'decaying'\n",
    "    elif 0.99 <= coeff <= 1:\n",
    "        return 'not decaying'\n",
    "    else:\n",
    "        return 'unknown'  # Handle any other cases here\n",
    "\n",
    "# Apply the custom function to create the \"decay_status\" column\n",
    "data['decay_status_kMc'] = data['kMc'].apply(get_decay_status)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "data['decay_status_kMc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decay_status_kMt\n",
       "decaying        6885\n",
       "not decaying    5049\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the custom function to create the \"decay_status\" column\n",
    "data['decay_status_kMt'] = data['kMt'].apply(get_decay_status)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "data['decay_status_kMt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution plot of 'kMc'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data['kMc'], bins=20, kde=True)\n",
    "plt.title('Distribution of kMc')\n",
    "plt.xlabel('kMc')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Create a pairplot grid\n",
    "sns.set(style=\"ticks\")\n",
    "#sns.pairplot(data, diag_kind='kde')\n",
    "#plt.suptitle('Pairplot of All Variables', y=1.02)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a simple MLP as a baseline to estimate compressor decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining features and target\n",
    "- splitting data into train, validation and test data, while shuffling the dataset and keeping distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Split features and target\n",
    "features = data.drop(columns=['kMc'])\n",
    "target = data['kMc']\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_size = 0.7\n",
    "val_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=1 - train_size, random_state=42, stratify=target, shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_size/(test_size + val_size), random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking for equal distribution between all three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decay_status_kMc\n",
       "done            878\n",
       "decaying        525\n",
       "not decaying    387\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.decay_status_kMc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decay_status_kMc\n",
       "done            4094\n",
       "decaying        2458\n",
       "not decaying    1801\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.decay_status_kMc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decay_status_kMc\n",
       "done            878\n",
       "decaying        527\n",
       "not decaying    386\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.decay_status_kMc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['decay_status_kMc', 'decay_status_kMt'])\n",
    "X_val = X_val.drop(columns=['decay_status_kMc', 'decay_status_kMt'])\n",
    "X_test = X_test.drop(columns=['decay_status_kMc', 'decay_status_kMt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the input data for the neural network to improve convergence and speed up training\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining different MLPs\n",
    "- starting with a simple MLP to train a regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "hidden_dim = 64  # Adjust this as needed\n",
    "output_dim = 1\n",
    "mlp_model = MLP(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "def train(model, epochs, X_train_scaled, y_train, X_val_tensor, y_val_tensor, lr=0.001, patience=10):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    inputs = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    targets = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        train_loss = criterion(outputs, targets)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val_tensor).squeeze()\n",
    "            val_loss = criterion(y_val_pred, y_val_tensor)\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "            \n",
    "        # keep count of training loss for early stopping\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        # check if early stopping criteria has been met\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([8353])) that is different to the input size (torch.Size([8353, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Train Loss: 0.0447, Val Loss: 0.0430\n",
      "Epoch [200/1000], Train Loss: 0.0040, Val Loss: 0.0040\n",
      "Epoch [300/1000], Train Loss: 0.0020, Val Loss: 0.0022\n",
      "Epoch [400/1000], Train Loss: 0.0013, Val Loss: 0.0014\n",
      "Epoch [500/1000], Train Loss: 0.0008, Val Loss: 0.0010\n",
      "Epoch [600/1000], Train Loss: 0.0006, Val Loss: 0.0008\n",
      "Epoch [700/1000], Train Loss: 0.0005, Val Loss: 0.0007\n",
      "Epoch [800/1000], Train Loss: 0.0004, Val Loss: 0.0007\n",
      "Epoch [900/1000], Train Loss: 0.0004, Val Loss: 0.0006\n",
      "Epoch [1000/1000], Train Loss: 0.0004, Val Loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "train(mlp_model, num_epochs, X_train_scaled, y_train, X_val_tensor, y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, X_test_scaled, y_test):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_inputs = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "        test_targets = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "        test_outputs = model(test_inputs)\n",
    "\n",
    "    mae = nn.L1Loss()\n",
    "    mae = mae(test_outputs, test_targets)\n",
    "    mse_o = nn.MSELoss()\n",
    "    mse = mse_o(test_outputs, test_targets)\n",
    "    rmse = torch.sqrt(mse_o(test_outputs, test_targets))\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.0159\n",
      "Mean Squared Error (MSE): 0.0004\n",
      "Root Mean Squared Error (RMSE): 0.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([1791])) that is different to the input size (torch.Size([1791, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Luca\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1791])) that is different to the input size (torch.Size([1791, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "evaluate(mlp_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>decay_status_kMc_target</th>\n",
       "      <th>decay_status_kMc_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.963233</td>\n",
       "      <td>not decaying</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.963</td>\n",
       "      <td>0.958043</td>\n",
       "      <td>done</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.955</td>\n",
       "      <td>0.957175</td>\n",
       "      <td>done</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.952</td>\n",
       "      <td>0.968837</td>\n",
       "      <td>done</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.970015</td>\n",
       "      <td>decaying</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  prediction decay_status_kMc_target decay_status_kMc_pred\n",
       "0   0.999    0.963233            not decaying                  done\n",
       "1   0.963    0.958043                    done                  done\n",
       "2   0.955    0.957175                    done                  done\n",
       "3   0.952    0.968837                    done                  done\n",
       "4   0.978    0.970015                decaying                  done"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "test_outputs = mlp_model(test_inputs)\n",
    "eval_df = pd.DataFrame({'target': y_test.tolist(), 'prediction': [value.item() for value in test_outputs]})\n",
    "eval_df['decay_status_kMc_target'] = eval_df['target'].apply(get_decay_status)\n",
    "eval_df['decay_status_kMc_pred'] = eval_df['prediction'].apply(get_decay_status)\n",
    "\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2803\n",
      "Recall: 0.2803\n",
      "Precision: 0.2600\n",
      "F1-score: 0.2655\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "y_true = eval_df['decay_status_kMc_target']\n",
    "y_pred = eval_df['decay_status_kMc_pred']\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):\n",
    "        super(MLP2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Train Loss: 0.0912, Val Loss: 0.0885\n",
      "Epoch [200/1000], Train Loss: 0.0033, Val Loss: 0.0031\n",
      "Epoch [300/1000], Train Loss: 0.0011, Val Loss: 0.0012\n",
      "Epoch [400/1000], Train Loss: 0.0007, Val Loss: 0.0008\n",
      "Epoch [500/1000], Train Loss: 0.0006, Val Loss: 0.0007\n",
      "Epoch [600/1000], Train Loss: 0.0005, Val Loss: 0.0006\n",
      "Epoch [700/1000], Train Loss: 0.0004, Val Loss: 0.0005\n",
      "Epoch [800/1000], Train Loss: 0.0004, Val Loss: 0.0005\n",
      "Epoch [900/1000], Train Loss: 0.0003, Val Loss: 0.0004\n",
      "Epoch [1000/1000], Train Loss: 0.0003, Val Loss: 0.0004\n",
      "Epoch [1100/1000], Train Loss: 0.0003, Val Loss: 0.0004\n",
      "Epoch [1200/1000], Train Loss: 0.0003, Val Loss: 0.0004\n",
      "Epoch [1300/1000], Train Loss: 0.0003, Val Loss: 0.0004\n",
      "Epoch [1400/1000], Train Loss: 0.0003, Val Loss: 0.0003\n",
      "Epoch [1500/1000], Train Loss: 0.0003, Val Loss: 0.0003\n",
      "Epoch [1600/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [1700/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [1800/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [1900/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2000/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2100/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2200/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2300/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2400/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2500/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2600/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2700/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [2800/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [2900/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3000/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3100/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3200/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3300/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3400/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3500/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3600/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3700/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3800/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3900/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4000/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4100/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4200/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4300/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4400/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4500/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4600/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4700/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4800/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4900/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [5000/1000], Train Loss: 0.0002, Val Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train_scaled.shape[1]\n",
    "hidden_dim1 = 24\n",
    "hidden_dim2 = 12\n",
    "hidden_dim3 = 6\n",
    "output_dim = 1\n",
    "\n",
    "mlp2_model = MLP2(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim)\n",
    "train(mlp2_model, 5000, X_train_scaled, y_train, X_val_tensor, y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp2_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m evaluate(\u001B[43mmlp2_model\u001B[49m, X_test_scaled, y_test)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'mlp2_model' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(mlp2_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- just increasing the size of the hidden layers does not work\n",
    "- adding an additional layer helps to converge faster, but without better results\n",
    "    - the bigger the network, the faster the convergence (early stopping is applied), but without better results on the test set\n",
    "- the RMSE is the same on the training, validation and test set, so overfitting isn't a problem (yet), is it?\n",
    "    - so early stopping based on the training loss shouln't be an issue right now, right?\n",
    "- increasing the learning rate to 0.01 leads to instant early stopping and underfitting\n",
    "- increasing the learning rate to 0.005 leads to way faster convergence with early stopping after ca. 400 epochs, but with similar performance on all datasets\n",
    "\n",
    "ToDo:\n",
    "- further increase network size\n",
    "- experiment with learning rate and batch size\n",
    "\n",
    "Question:\n",
    "- are there other neural network types suitable for this problem?\n",
    "    - LSTM, attention and CNN are not suitable imo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, hidden_dim5, output_dim):\n",
    "        super(MLP3, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        self.fc5 = nn.Linear(hidden_dim4, hidden_dim5)\n",
    "        self.fc6 = nn.Linear(hidden_dim5, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/5000], Train Loss: 0.0003, Val Loss: 0.0004\n",
      "Epoch [200/5000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [300/5000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [400/5000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [500/5000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [600/5000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [700/5000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [800/5000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [900/5000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [1000/5000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train_scaled.shape[1]\n",
    "hidden_dim1 = 192\n",
    "hidden_dim2 = 288\n",
    "hidden_dim3 = 192\n",
    "hidden_dim4 = 96\n",
    "hidden_dim5 = 24\n",
    "output_dim = 1\n",
    "\n",
    "mlp3_model = MLP3(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, hidden_dim5, output_dim)\n",
    "train(mlp3_model, 5000, X_train_scaled, y_train, X_val_tensor, y_val_tensor, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.0128\n",
      "Mean Squared Error (MSE): 0.0002\n",
      "Root Mean Squared Error (RMSE): 0.0148\n"
     ]
    }
   ],
   "source": [
    "evaluate(mlp3_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.0029602187\n",
      "Mean Squared Error: 0.0039781134\n",
      "Root Mean Squared Error: 0.0000158254\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Create an XGBRegressor model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,  # Number of boosting rounds\n",
    "    learning_rate=0.1,  # Learning rate\n",
    "    max_depth=3,  # Maximum depth of each tree\n",
    "    objective='reg:squarederror',  # Regression task\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the target variable on the test data\n",
    "y_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) as a performance metric\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=True)\n",
    "\n",
    "print(\"Mean Absolute Error:\", \"{:.10f}\".format(mae))\n",
    "print(\"Mean Squared Error:\", \"{:.10f}\".format(mse))\n",
    "print(\"Root Mean Squared Error:\", \"{:.10f}\".format(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}