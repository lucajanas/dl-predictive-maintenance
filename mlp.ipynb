{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Predictive Maintenance\n",
    "Data set: https://archive.ics.uci.edu/dataset/316/condition+based+maintenance+of+naval+propulsion+plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lp</th>\n",
       "      <th>v</th>\n",
       "      <th>GTT</th>\n",
       "      <th>GTn</th>\n",
       "      <th>GGn</th>\n",
       "      <th>Ts</th>\n",
       "      <th>Tp</th>\n",
       "      <th>T48</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>P48</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>Pexh</th>\n",
       "      <th>TIC</th>\n",
       "      <th>mf</th>\n",
       "      <th>kMc</th>\n",
       "      <th>kMt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.138</td>\n",
       "      <td>3</td>\n",
       "      <td>289.964</td>\n",
       "      <td>1349.489</td>\n",
       "      <td>6677.380</td>\n",
       "      <td>7.584</td>\n",
       "      <td>7.584</td>\n",
       "      <td>464.006</td>\n",
       "      <td>288</td>\n",
       "      <td>550.563</td>\n",
       "      <td>1.096</td>\n",
       "      <td>0.998</td>\n",
       "      <td>5.947</td>\n",
       "      <td>1.019</td>\n",
       "      <td>7.137</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.088</td>\n",
       "      <td>6</td>\n",
       "      <td>6960.180</td>\n",
       "      <td>1376.166</td>\n",
       "      <td>6828.469</td>\n",
       "      <td>28.204</td>\n",
       "      <td>28.204</td>\n",
       "      <td>635.401</td>\n",
       "      <td>288</td>\n",
       "      <td>581.658</td>\n",
       "      <td>1.331</td>\n",
       "      <td>0.998</td>\n",
       "      <td>7.282</td>\n",
       "      <td>1.019</td>\n",
       "      <td>10.655</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.144</td>\n",
       "      <td>9</td>\n",
       "      <td>8379.229</td>\n",
       "      <td>1386.757</td>\n",
       "      <td>7111.811</td>\n",
       "      <td>60.358</td>\n",
       "      <td>60.358</td>\n",
       "      <td>606.002</td>\n",
       "      <td>288</td>\n",
       "      <td>587.587</td>\n",
       "      <td>1.389</td>\n",
       "      <td>0.998</td>\n",
       "      <td>7.574</td>\n",
       "      <td>1.020</td>\n",
       "      <td>13.086</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.161</td>\n",
       "      <td>12</td>\n",
       "      <td>14724.395</td>\n",
       "      <td>1547.465</td>\n",
       "      <td>7792.630</td>\n",
       "      <td>113.774</td>\n",
       "      <td>113.774</td>\n",
       "      <td>661.471</td>\n",
       "      <td>288</td>\n",
       "      <td>613.851</td>\n",
       "      <td>1.658</td>\n",
       "      <td>0.998</td>\n",
       "      <td>9.007</td>\n",
       "      <td>1.022</td>\n",
       "      <td>18.109</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.140</td>\n",
       "      <td>15</td>\n",
       "      <td>21636.432</td>\n",
       "      <td>1924.313</td>\n",
       "      <td>8494.777</td>\n",
       "      <td>175.306</td>\n",
       "      <td>175.306</td>\n",
       "      <td>731.494</td>\n",
       "      <td>288</td>\n",
       "      <td>645.642</td>\n",
       "      <td>2.078</td>\n",
       "      <td>0.998</td>\n",
       "      <td>11.197</td>\n",
       "      <td>1.026</td>\n",
       "      <td>26.373</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lp   v        GTT       GTn       GGn       Ts       Tp      T48   T1  \\\n",
       "0  1.138   3    289.964  1349.489  6677.380    7.584    7.584  464.006  288   \n",
       "1  2.088   6   6960.180  1376.166  6828.469   28.204   28.204  635.401  288   \n",
       "2  3.144   9   8379.229  1386.757  7111.811   60.358   60.358  606.002  288   \n",
       "3  4.161  12  14724.395  1547.465  7792.630  113.774  113.774  661.471  288   \n",
       "4  5.140  15  21636.432  1924.313  8494.777  175.306  175.306  731.494  288   \n",
       "\n",
       "        T2    P48     P1      P2   Pexh     TIC     mf   kMc    kMt  \n",
       "0  550.563  1.096  0.998   5.947  1.019   7.137  0.082  0.95  0.975  \n",
       "1  581.658  1.331  0.998   7.282  1.019  10.655  0.287  0.95  0.975  \n",
       "2  587.587  1.389  0.998   7.574  1.020  13.086  0.259  0.95  0.975  \n",
       "3  613.851  1.658  0.998   9.007  1.022  18.109  0.358  0.95  0.975  \n",
       "4  645.642  2.078  0.998  11.197  1.026  26.373  0.522  0.95  0.975  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['lp','v','GTT','GTn','GGn','Ts','Tp','T48','T1','T2','P48','P1','P2','Pexh','TIC','mf','kMc','kMt']\n",
    "data = pd.read_csv('data/data.csv', names=columns, skiprows=1)\n",
    "\n",
    "# kMc = GT Compressor decay state coefficient, kMt = GT Turbine decay state coefficient\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11934, 18)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lp</th>\n",
       "      <th>v</th>\n",
       "      <th>GTT</th>\n",
       "      <th>GTn</th>\n",
       "      <th>GGn</th>\n",
       "      <th>Ts</th>\n",
       "      <th>Tp</th>\n",
       "      <th>T48</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>P48</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>Pexh</th>\n",
       "      <th>TIC</th>\n",
       "      <th>mf</th>\n",
       "      <th>kMc</th>\n",
       "      <th>kMt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.0</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "      <td>11934.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.167</td>\n",
       "      <td>15.000</td>\n",
       "      <td>27247.499</td>\n",
       "      <td>2136.289</td>\n",
       "      <td>8200.947</td>\n",
       "      <td>227.336</td>\n",
       "      <td>227.336</td>\n",
       "      <td>735.495</td>\n",
       "      <td>288.0</td>\n",
       "      <td>646.215</td>\n",
       "      <td>2.353</td>\n",
       "      <td>0.998</td>\n",
       "      <td>12.297</td>\n",
       "      <td>1.029</td>\n",
       "      <td>33.641</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.626</td>\n",
       "      <td>7.746</td>\n",
       "      <td>22148.613</td>\n",
       "      <td>774.084</td>\n",
       "      <td>1091.316</td>\n",
       "      <td>200.496</td>\n",
       "      <td>200.496</td>\n",
       "      <td>173.681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.676</td>\n",
       "      <td>1.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.337</td>\n",
       "      <td>0.010</td>\n",
       "      <td>25.841</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.138</td>\n",
       "      <td>3.000</td>\n",
       "      <td>253.547</td>\n",
       "      <td>1307.675</td>\n",
       "      <td>6589.002</td>\n",
       "      <td>5.304</td>\n",
       "      <td>5.304</td>\n",
       "      <td>442.364</td>\n",
       "      <td>288.0</td>\n",
       "      <td>540.442</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.998</td>\n",
       "      <td>5.828</td>\n",
       "      <td>1.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.144</td>\n",
       "      <td>9.000</td>\n",
       "      <td>8375.884</td>\n",
       "      <td>1386.758</td>\n",
       "      <td>7058.324</td>\n",
       "      <td>60.317</td>\n",
       "      <td>60.317</td>\n",
       "      <td>589.873</td>\n",
       "      <td>288.0</td>\n",
       "      <td>578.092</td>\n",
       "      <td>1.389</td>\n",
       "      <td>0.998</td>\n",
       "      <td>7.447</td>\n",
       "      <td>1.020</td>\n",
       "      <td>13.678</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.140</td>\n",
       "      <td>15.000</td>\n",
       "      <td>21630.659</td>\n",
       "      <td>1924.326</td>\n",
       "      <td>8482.082</td>\n",
       "      <td>175.268</td>\n",
       "      <td>175.268</td>\n",
       "      <td>706.038</td>\n",
       "      <td>288.0</td>\n",
       "      <td>637.142</td>\n",
       "      <td>2.083</td>\n",
       "      <td>0.998</td>\n",
       "      <td>11.092</td>\n",
       "      <td>1.026</td>\n",
       "      <td>25.276</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.148</td>\n",
       "      <td>21.000</td>\n",
       "      <td>39001.427</td>\n",
       "      <td>2678.079</td>\n",
       "      <td>9132.606</td>\n",
       "      <td>332.365</td>\n",
       "      <td>332.365</td>\n",
       "      <td>834.066</td>\n",
       "      <td>288.0</td>\n",
       "      <td>693.924</td>\n",
       "      <td>2.981</td>\n",
       "      <td>0.998</td>\n",
       "      <td>15.658</td>\n",
       "      <td>1.036</td>\n",
       "      <td>44.552</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.300</td>\n",
       "      <td>27.000</td>\n",
       "      <td>72784.872</td>\n",
       "      <td>3560.741</td>\n",
       "      <td>9797.103</td>\n",
       "      <td>645.249</td>\n",
       "      <td>645.249</td>\n",
       "      <td>1115.797</td>\n",
       "      <td>288.0</td>\n",
       "      <td>789.094</td>\n",
       "      <td>4.560</td>\n",
       "      <td>0.998</td>\n",
       "      <td>23.140</td>\n",
       "      <td>1.052</td>\n",
       "      <td>92.556</td>\n",
       "      <td>1.832</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              lp          v        GTT        GTn        GGn         Ts  \\\n",
       "count  11934.000  11934.000  11934.000  11934.000  11934.000  11934.000   \n",
       "mean       5.167     15.000  27247.499   2136.289   8200.947    227.336   \n",
       "std        2.626      7.746  22148.613    774.084   1091.316    200.496   \n",
       "min        1.138      3.000    253.547   1307.675   6589.002      5.304   \n",
       "25%        3.144      9.000   8375.884   1386.758   7058.324     60.317   \n",
       "50%        5.140     15.000  21630.659   1924.326   8482.082    175.268   \n",
       "75%        7.148     21.000  39001.427   2678.079   9132.606    332.365   \n",
       "max        9.300     27.000  72784.872   3560.741   9797.103    645.249   \n",
       "\n",
       "              Tp        T48       T1         T2        P48         P1  \\\n",
       "count  11934.000  11934.000  11934.0  11934.000  11934.000  11934.000   \n",
       "mean     227.336    735.495    288.0    646.215      2.353      0.998   \n",
       "std      200.496    173.681      0.0     72.676      1.085      0.000   \n",
       "min        5.304    442.364    288.0    540.442      1.093      0.998   \n",
       "25%       60.317    589.873    288.0    578.092      1.389      0.998   \n",
       "50%      175.268    706.038    288.0    637.142      2.083      0.998   \n",
       "75%      332.365    834.066    288.0    693.924      2.981      0.998   \n",
       "max      645.249   1115.797    288.0    789.094      4.560      0.998   \n",
       "\n",
       "              P2       Pexh        TIC         mf        kMc        kMt  \n",
       "count  11934.000  11934.000  11934.000  11934.000  11934.000  11934.000  \n",
       "mean      12.297      1.029     33.641      0.662      0.975      0.988  \n",
       "std        5.337      0.010     25.841      0.507      0.015      0.008  \n",
       "min        5.828      1.019      0.000      0.068      0.950      0.975  \n",
       "25%        7.447      1.020     13.678      0.246      0.962      0.981  \n",
       "50%       11.092      1.026     25.276      0.496      0.975      0.988  \n",
       "75%       15.658      1.036     44.552      0.882      0.988      0.994  \n",
       "max       23.140      1.052     92.556      1.832      1.000      1.000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.95\n",
       "1        0.95\n",
       "2        0.95\n",
       "3        0.95\n",
       "4        0.95\n",
       "         ... \n",
       "11929    1.00\n",
       "11930    1.00\n",
       "11931    1.00\n",
       "11932    1.00\n",
       "11933    1.00\n",
       "Name: kMc, Length: 11934, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.kMc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decay_status_kMc\n",
       "done            5850\n",
       "decaying        3510\n",
       "not decaying    2574\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a custom function to determine decay status\n",
    "def get_decay_status(coeff):\n",
    "    if coeff < 0.975:\n",
    "        return 'done'\n",
    "    elif 0.975 <= coeff < 0.99:\n",
    "        return 'decaying'\n",
    "    elif 0.99 <= coeff <= 1:\n",
    "        return 'not decaying'\n",
    "    else:\n",
    "        return 'unknown'  # Handle any other cases here\n",
    "\n",
    "# Apply the custom function to create the \"decay_status\" column\n",
    "data['decay_status_kMc'] = data['kMc'].apply(get_decay_status)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "data['decay_status_kMc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decay_status_kMt\n",
       "decaying        6885\n",
       "not decaying    5049\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the custom function to create the \"decay_status\" column\n",
    "data['decay_status_kMt'] = data['kMt'].apply(get_decay_status)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "data['decay_status_kMt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution plot of 'kMc'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data['kMc'], bins=20, kde=True)\n",
    "plt.title('Distribution of kMc')\n",
    "plt.xlabel('kMc')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Create a pairplot grid\n",
    "sns.set(style=\"ticks\")\n",
    "#sns.pairplot(data, diag_kind='kde')\n",
    "#plt.suptitle('Pairplot of All Variables', y=1.02)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a simple MLP as a baseline to estimate compressor decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Split features and target\n",
    "features = data.drop(columns=['kMc'])\n",
    "target = data['kMc']\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_size = 0.7\n",
    "val_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=1 - train_size, random_state=42, stratify=target)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_size/(test_size + val_size), random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decay_status_kMc\n",
       "done            878\n",
       "decaying        525\n",
       "not decaying    387\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.decay_status_kMc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decay_status_kMc\n",
       "done            4094\n",
       "decaying        2458\n",
       "not decaying    1801\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.decay_status_kMc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decay_status_kMc\n",
       "done            878\n",
       "decaying        527\n",
       "not decaying    386\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.decay_status_kMc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['decay_status_kMc', 'decay_status_kMt'])\n",
    "X_val = X_val.drop(columns=['decay_status_kMc', 'decay_status_kMt'])\n",
    "X_test = X_test.drop(columns=['decay_status_kMc', 'decay_status_kMt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "hidden_dim = 64  # Adjust this as needed\n",
    "output_dim = 1\n",
    "mlp_model = MLP(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "def train(model, epochs, X_train_scaled, y_train, X_val_tensor, y_val_tensor, lr=0.001, patience=10):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    inputs = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    targets = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        train_loss = criterion(outputs, targets)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val_tensor).squeeze()\n",
    "            val_loss = criterion(y_val_pred, y_val_tensor)\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "            \n",
    "        # keep count of training loss for early stopping\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        # check if early stopping criteria has been met\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([8353])) that is different to the input size (torch.Size([8353, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Train Loss: 0.0447, Val Loss: 0.0430\n",
      "Epoch [200/1000], Train Loss: 0.0040, Val Loss: 0.0040\n",
      "Epoch [300/1000], Train Loss: 0.0020, Val Loss: 0.0022\n",
      "Epoch [400/1000], Train Loss: 0.0013, Val Loss: 0.0014\n",
      "Epoch [500/1000], Train Loss: 0.0008, Val Loss: 0.0010\n",
      "Epoch [600/1000], Train Loss: 0.0006, Val Loss: 0.0008\n",
      "Epoch [700/1000], Train Loss: 0.0005, Val Loss: 0.0007\n",
      "Epoch [800/1000], Train Loss: 0.0004, Val Loss: 0.0007\n",
      "Epoch [900/1000], Train Loss: 0.0004, Val Loss: 0.0006\n",
      "Epoch [1000/1000], Train Loss: 0.0004, Val Loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "train(mlp_model, num_epochs, X_train_scaled, y_train, X_val_tensor, y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, X_test_scaled, y_test):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_inputs = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "        test_targets = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "        test_outputs = model(test_inputs)\n",
    "\n",
    "    mae = nn.L1Loss()\n",
    "    mae = mae(test_outputs, test_targets)\n",
    "    mse_o = nn.MSELoss()\n",
    "    mse = mse_o(test_outputs, test_targets)\n",
    "    rmse = torch.sqrt(mse_o(test_outputs, test_targets))\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.0159\n",
      "Mean Squared Error (MSE): 0.0004\n",
      "Root Mean Squared Error (RMSE): 0.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([1791])) that is different to the input size (torch.Size([1791, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Luca\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1791])) that is different to the input size (torch.Size([1791, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "evaluate(mlp_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>decay_status_kMc_target</th>\n",
       "      <th>decay_status_kMc_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.963233</td>\n",
       "      <td>not decaying</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.963</td>\n",
       "      <td>0.958043</td>\n",
       "      <td>done</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.955</td>\n",
       "      <td>0.957175</td>\n",
       "      <td>done</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.952</td>\n",
       "      <td>0.968837</td>\n",
       "      <td>done</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.970015</td>\n",
       "      <td>decaying</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  prediction decay_status_kMc_target decay_status_kMc_pred\n",
       "0   0.999    0.963233            not decaying                  done\n",
       "1   0.963    0.958043                    done                  done\n",
       "2   0.955    0.957175                    done                  done\n",
       "3   0.952    0.968837                    done                  done\n",
       "4   0.978    0.970015                decaying                  done"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "test_outputs = mlp_model(test_inputs)\n",
    "eval_df = pd.DataFrame({'target': y_test.tolist(), 'prediction': [value.item() for value in test_outputs]})\n",
    "eval_df['decay_status_kMc_target'] = eval_df['target'].apply(get_decay_status)\n",
    "eval_df['decay_status_kMc_pred'] = eval_df['prediction'].apply(get_decay_status)\n",
    "\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2803\n",
      "Recall: 0.2803\n",
      "Precision: 0.2600\n",
      "F1-score: 0.2655\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "y_true = eval_df['decay_status_kMc_target']\n",
    "y_pred = eval_df['decay_status_kMc_pred']\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "Next, a LSTM is trained and evaluated on the scaled input data. Due to not having a real sequence of data, it is expected that using LSTM does not give ideal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_scaled.shape[1]\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "lstm_model = LSTM(input_dim, hidden_dim, num_layers, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(lstm_model, num_epochs, X_train_scaled, y_train, X_val_tensor, y_val_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Train Loss: 0.9210, Val Loss: 0.9291\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "class AttentionMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(AttentionMLP, self).__init__()\n",
    "        \n",
    "        self.attention_weights = nn.Parameter(torch.Tensor(input_dim, 1))\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.attention_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_scores = torch.matmul(x, self.attention_weights)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        attended_features = x * attention_weights\n",
    "        output = self.mlp(attended_features)\n",
    "        return output\n",
    "\n",
    "# Define the AttentionMLP model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "attention_mlp_model = AttentionMLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Train and evaluate the AttentionMLP model using your train and evaluate functions\n",
    "num_epochs = 1000\n",
    "train(attention_mlp_model, num_epochs, X_train_scaled, y_train, X_val_tensor, y_val_tensor, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 1.1800\n",
      "Mean Squared Error (MSE): 1.4845\n",
      "Root Mean Squared Error (RMSE): 1.2184\n"
     ]
    }
   ],
   "source": [
    "evaluate(attention_mlp_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):\n",
    "        super(MLP2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Train Loss: 0.0912, Val Loss: 0.0885\n",
      "Epoch [200/1000], Train Loss: 0.0033, Val Loss: 0.0031\n",
      "Epoch [300/1000], Train Loss: 0.0011, Val Loss: 0.0012\n",
      "Epoch [400/1000], Train Loss: 0.0007, Val Loss: 0.0008\n",
      "Epoch [500/1000], Train Loss: 0.0006, Val Loss: 0.0007\n",
      "Epoch [600/1000], Train Loss: 0.0005, Val Loss: 0.0006\n",
      "Epoch [700/1000], Train Loss: 0.0004, Val Loss: 0.0005\n",
      "Epoch [800/1000], Train Loss: 0.0004, Val Loss: 0.0005\n",
      "Epoch [900/1000], Train Loss: 0.0003, Val Loss: 0.0004\n",
      "Epoch [1000/1000], Train Loss: 0.0003, Val Loss: 0.0004\n",
      "Epoch [1100/1000], Train Loss: 0.0003, Val Loss: 0.0004\n",
      "Epoch [1200/1000], Train Loss: 0.0003, Val Loss: 0.0004\n",
      "Epoch [1300/1000], Train Loss: 0.0003, Val Loss: 0.0004\n",
      "Epoch [1400/1000], Train Loss: 0.0003, Val Loss: 0.0003\n",
      "Epoch [1500/1000], Train Loss: 0.0003, Val Loss: 0.0003\n",
      "Epoch [1600/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [1700/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [1800/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [1900/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2000/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2100/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2200/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2300/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2400/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2500/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2600/1000], Train Loss: 0.0002, Val Loss: 0.0003\n",
      "Epoch [2700/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [2800/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [2900/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3000/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3100/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3200/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3300/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3400/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3500/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3600/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3700/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3800/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [3900/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4000/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4100/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4200/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4300/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4400/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4500/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4600/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4700/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4800/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [4900/1000], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [5000/1000], Train Loss: 0.0002, Val Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train_scaled.shape[1]\n",
    "hidden_dim1 = 24\n",
    "hidden_dim2 = 12\n",
    "hidden_dim3 = 6\n",
    "output_dim = 1\n",
    "\n",
    "mlp2_model = MLP2(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim)\n",
    "train(mlp2_model, 5000, X_train_scaled, y_train, X_val_tensor, y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.0128\n",
      "Mean Squared Error (MSE): 0.0002\n",
      "Root Mean Squared Error (RMSE): 0.0148\n"
     ]
    }
   ],
   "source": [
    "evaluate(mlp2_model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url: https://saturncloud.io/blog/convolution-neural-network-for-regression-using-pytorch/\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=8353, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=32 * 7 * 7, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train(cnn_model, 1000, X_train_scaled, y_train, X_val_tensor, y_val_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.0029602187\n",
      "Mean Squared Error: 0.0039781134\n",
      "Root Mean Squared Error: 0.0000158254\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Create an XGBRegressor model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,  # Number of boosting rounds\n",
    "    learning_rate=0.1,  # Learning rate\n",
    "    max_depth=3,  # Maximum depth of each tree\n",
    "    objective='reg:squarederror',  # Regression task\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the target variable on the test data\n",
    "y_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) as a performance metric\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=True)\n",
    "\n",
    "print(\"Mean Absolute Error:\", \"{:.10f}\".format(mae))\n",
    "print(\"Mean Squared Error:\", \"{:.10f}\".format(mse))\n",
    "print(\"Root Mean Squared Error:\", \"{:.10f}\".format(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}